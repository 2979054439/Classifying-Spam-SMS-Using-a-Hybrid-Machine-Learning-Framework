# æ ¸å¿ƒä¾èµ–ï¼špandas, numpy, matplotlib, scikit-learn, nltkï¼ˆå·²é€šè¿‡pipå®‰è£…ï¼‰
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import classification_report, confusion_matrix, silhouette_score, adjusted_rand_score
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
import nltk
from nltk.corpus import stopwords
import re
import matplotlib
matplotlib.font_manager.fontManager.addfont("C:/Windows/Fonts/msyh.ttc")  # å¾®è½¯é›…é»‘
matplotlib.rcParams['font.sans-serif'] = ['Microsoft YaHei']
matplotlib.rcParams['axes.unicode_minus'] = False

# ä¸‹è½½NLTKåœç”¨è¯ï¼ˆä»…é¦–æ¬¡è¿è¡Œéœ€è¦ï¼Œä¸‹è½½ä¸€æ¬¡åä¸‹æ¬¡æ— éœ€é‡å¤ï¼‰
try:
    stopwords.words('english')  # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
except LookupError:
    nltk.download('stopwords')

# --------------------------
# 1. åŠ è½½æ•°æ®é›†ï¼ˆç¡®ä¿è·¯å¾„æ­£ç¡®ï¼‰
# --------------------------
def load_data(file_path):
    """åŠ è½½SMSåƒåœ¾çŸ­ä¿¡æ•°æ®é›†"""
    try:
        df = pd.read_csv(file_path, sep='\t', names=['label', 'text'], encoding='utf-8')
        print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸï¼å½¢çŠ¶: {df.shape}")
        print(f"æ ‡ç­¾åˆ†å¸ƒ:\n{df['label'].value_counts()}\n")
        return df
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {file_path}")
        print("è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œä¾‹å¦‚ï¼š")
        print("Windows: r'C:\\Users\\ä½ çš„ç”¨æˆ·å\\æ¡Œé¢\\sms\\SMSSpamCollection.txt'")
        print("Mac/Linux: '/Users/ä½ çš„ç”¨æˆ·å/Desktop/sms/SMSSpamCollection.txt'")
        exit()

# ğŸ”´ è¯·æ ¹æ®ä½ çš„å®é™…æ–‡ä»¶è·¯å¾„ä¿®æ”¹è¿™é‡Œï¼
file_path = r"D:\Program Files\SMSSpamCollection"
df = load_data(file_path)

# --------------------------
# 2. æ–‡æœ¬æ¸…æ´— + æ ‡ç­¾ç¼–ç 
# --------------------------
def clean_text(text):
    """æ–‡æœ¬æ¸…æ´—ï¼šå°å†™ã€ç§»é™¤ç‰¹æ®Šå­—ç¬¦/æ•°å­—ã€å»å¤šä½™ç©ºæ ¼"""
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # åªä¿ç•™å­—æ¯å’Œç©ºæ ¼
    text = re.sub(r'\s+', ' ', text).strip()  # åˆå¹¶è¿ç»­ç©ºæ ¼
    return text

# åº”ç”¨æ¸…æ´—
df['cleaned_text'] = df['text'].apply(clean_text)

# æ ‡ç­¾ç¼–ç ï¼ˆham=0ï¼Œspam=1ï¼‰
le = LabelEncoder()
df['label_encoded'] = le.fit_transform(df['label'])

# --------------------------
# 3. TF-IDFç‰¹å¾æå–
# --------------------------
tfidf_vectorizer = TfidfVectorizer(
    max_features=5000,  # ä¿ç•™Top5000é«˜é¢‘è¯
    stop_words=stopwords.words('english'),  # ç§»é™¤åœç”¨è¯ï¼ˆå¦‚the, andï¼‰
    ngram_range=(1, 2),  # ä¿ç•™å•å­—ï¼ˆå¦‚"free"ï¼‰å’ŒåŒå­—ï¼ˆå¦‚"free gift"ï¼‰
    min_df=2,  # è‡³å°‘å‡ºç°2æ¬¡çš„è¯æ‰ä¿ç•™
    max_df=0.95  # è¿‡æ»¤è¿‡äºå¸¸è§çš„è¯ï¼ˆå‡ºç°ç‡>95%ï¼‰
)

# ç”Ÿæˆç‰¹å¾çŸ©é˜µï¼ˆå…³é”®ï¼šå…ˆåˆ›å»ºå†ä½¿ç”¨ï¼‰
X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_text'])
y = df['label_encoded']
print(f"âœ… TF-IDFç‰¹å¾çŸ©é˜µç”ŸæˆæˆåŠŸï¼å½¢çŠ¶: {X_tfidf.shape}ï¼ˆæ ·æœ¬æ•° Ã— ç‰¹å¾æ•°ï¼‰\n")

# --------------------------
# 4. åˆ†ç±»ä»»åŠ¡ï¼ˆç›‘ç£å­¦ä¹ ï¼‰
# --------------------------
# åˆ†å‰²è®­ç»ƒé›†/æµ‹è¯•é›†ï¼ˆåˆ†å±‚æŠ½æ ·ï¼Œä¿è¯æ ‡ç­¾åˆ†å¸ƒä¸€è‡´ï¼‰
X_train, X_test, y_train, y_test = train_test_split(
    X_tfidf, y, test_size=0.3, random_state=42, stratify=y
)

# åˆå§‹åŒ–3ä¸ªç»å…¸åˆ†ç±»å™¨
classifiers = {
    'æœ´ç´ è´å¶æ–¯': MultinomialNB(),  # è½»é‡ã€å¿«é€Ÿï¼Œé€‚åˆæ–‡æœ¬åˆ†ç±»
    'çº¿æ€§SVM': SVC(kernel='linear', probability=True, random_state=42),  # åˆ†ç±»æ•ˆæœå¥½
    'éšæœºæ£®æ—': RandomForestClassifier(n_estimators=100, random_state=42)  # é²æ£’æ€§å¼º
}

# è®­ç»ƒå¹¶è¯„ä¼°åˆ†ç±»å™¨
def evaluate_classifiers(classifiers, X_train, X_test, y_train, y_test):
    results = {}
    for name, clf in classifiers.items():
        clf.fit(X_train, y_train)  # è®­ç»ƒ
        y_pred = clf.predict(X_test)  # é¢„æµ‹
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        report = classification_report(y_test, y_pred, target_names=['æ­£å¸¸çŸ­ä¿¡', 'åƒåœ¾çŸ­ä¿¡'], output_dict=True)
        cm = confusion_matrix(y_test, y_pred)
        
        # ä¿å­˜ç»“æœ
        results[name] = {
            'å‡†ç¡®ç‡': report['accuracy'],
            'ç²¾ç¡®ç‡': report['weighted avg']['precision'],
            'å¬å›ç‡': report['weighted avg']['recall'],
            'F1åˆ†æ•°': report['weighted avg']['f1-score'],
            'æ··æ·†çŸ©é˜µ': cm
        }
        
        # æ‰“å°ç»“æœ
        print(f"=== {name} æ€§èƒ½ ===")
        print(f"å‡†ç¡®ç‡: {report['accuracy']:.4f}")
        print(f"F1åˆ†æ•°: {report['weighted avg']['f1-score']:.4f}\n")
    
    return results

# æ‰§è¡Œåˆ†ç±»è¯„ä¼°
print("ğŸ“Š å¼€å§‹è®­ç»ƒåˆ†ç±»å™¨...")
classification_results = evaluate_classifiers(classifiers, X_train, X_test, y_train, y_test)

# --------------------------
# 5. åˆ†ç±»ç»“æœå¯è§†åŒ–ï¼ˆçº¯matplotlibï¼‰
# --------------------------
# æ··æ·†çŸ©é˜µå¯è§†åŒ–
fig, axes = plt.subplots(1, 3, figsize=(18, 5))
colors = ['#1f77b4', '#ff7f0e', '#2ca02c']  # æ¯ä¸ªåˆ†ç±»å™¨çš„é¢œè‰²

for idx, (name, result) in enumerate(classification_results.items()):
    cm = result['æ··æ·†çŸ©é˜µ']
    # ç»˜åˆ¶çƒ­åŠ›å›¾
    im = axes[idx].imshow(cm, cmap='Blues', alpha=0.8)
    # æ·»åŠ æ•°å€¼æ ‡æ³¨
    for i in range(2):
        for j in range(2):
            axes[idx].text(j, i, str(cm[i, j]), ha='center', va='center', fontsize=14, fontweight='bold')
    # è®¾ç½®æ ‡ç­¾
    axes[idx].set_xticks([0, 1])
    axes[idx].set_yticks([0, 1])
    axes[idx].set_xticklabels(['æ­£å¸¸çŸ­ä¿¡', 'åƒåœ¾çŸ­ä¿¡'])
    axes[idx].set_yticklabels(['æ­£å¸¸çŸ­ä¿¡', 'åƒåœ¾çŸ­ä¿¡'])
    axes[idx].set_title(f'{name} - æ··æ·†çŸ©é˜µ', fontsize=12)
    axes[idx].set_xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=10)
    axes[idx].set_ylabel('çœŸå®æ ‡ç­¾', fontsize=10)
    # é¢œè‰²æ¡
    plt.colorbar(im, ax=axes[idx], shrink=0.7)

plt.tight_layout()
plt.show()

# åˆ†ç±»å™¨æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾
metrics_df = pd.DataFrame(classification_results).T[['å‡†ç¡®ç‡', 'ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1åˆ†æ•°']]
metrics_df.plot(kind='bar', figsize=(10, 6), rot=0, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])
plt.title('åˆ†ç±»å™¨æ€§èƒ½å¯¹æ¯”', fontsize=14)
plt.ylabel('åˆ†æ•°', fontsize=12)
plt.ylim(0.9, 1.0)  # æ”¾å¤§å·®å¼‚
plt.grid(axis='y', alpha=0.3)
plt.legend(loc='upper right')
plt.tight_layout()
plt.show()

# --------------------------
# 6. èšç±»ä»»åŠ¡ï¼ˆæ— ç›‘ç£å­¦ä¹ ï¼‰
# --------------------------
# PCAé™ç»´ï¼ˆé«˜ç»´ç‰¹å¾â†’2ç»´ï¼Œç”¨äºå¯è§†åŒ–ï¼‰
print("ğŸ” å¼€å§‹èšç±»åˆ†æ...")
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X_tfidf.toarray())  # ç¨€ç–çŸ©é˜µè½¬ç¨ å¯†çŸ©é˜µ
print(f"PCAé™ç»´å®Œæˆï¼ç´¯è®¡è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_.sum():.3f}\n")

# æ„å»ºPCAç»“æœDataFrame
pca_df = pd.DataFrame({
    'PC1': X_pca[:, 0],
    'PC2': X_pca[:, 1],
    'çœŸå®æ ‡ç­¾': df['label'],
    'æ ‡ç­¾ç¼–ç ': df['label_encoded']
})

# å¯»æ‰¾K-meansæœ€ä½³Kå€¼ï¼ˆè‚˜éƒ¨æ³•åˆ™+è½®å»“ç³»æ•°ï¼‰
inertia = []  # ç°‡å†…å¹³æ–¹å’Œï¼ˆè¶Šå°è¶Šå¥½ï¼‰
silhouette_scores = []  # è½®å»“ç³»æ•°ï¼ˆè¶Šæ¥è¿‘1è¶Šå¥½ï¼‰
k_range = range(2, 8)

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_tfidf)
    inertia.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(X_tfidf, kmeans.labels_))

# å¯è§†åŒ–æœ€ä½³Kå€¼
plt.figure(figsize=(12, 4))
# è‚˜éƒ¨æ³•åˆ™å›¾
plt.subplot(1, 2, 1)
plt.plot(k_range, inertia, 'bo-', linewidth=2)
plt.xlabel('èšç±»æ•°é‡ k')
plt.ylabel('ç°‡å†…å¹³æ–¹å’Œ')
plt.title('è‚˜éƒ¨æ³•åˆ™ï¼ˆæ‰¾æœ€ä½³kï¼‰')
plt.grid(alpha=0.3)

# è½®å»“ç³»æ•°å›¾
plt.subplot(1, 2, 2)
plt.plot(k_range, silhouette_scores, 'ro-', linewidth=2)
plt.xlabel('èšç±»æ•°é‡ k')
plt.ylabel('è½®å»“ç³»æ•°')
plt.title('è½®å»“ç³»æ•°åˆ†æï¼ˆæ‰¾æœ€ä½³kï¼‰')
plt.grid(alpha=0.3)

plt.tight_layout()
plt.show()

# æœ€ä½³K=2ï¼ˆå¯¹åº”æ­£å¸¸/åƒåœ¾çŸ­ä¿¡ï¼‰
optimal_k = 2
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
kmeans_labels = kmeans.fit_predict(X_tfidf)

# èšç±»æ•ˆæœè¯„ä¼°
print(f"=== K-meansèšç±»ç»“æœï¼ˆk={optimal_k}ï¼‰===")
print(f"è½®å»“ç³»æ•°: {silhouette_score(X_tfidf, kmeans_labels):.4f}ï¼ˆè¶Šæ¥è¿‘1è¶Šå¥½ï¼‰")
print(f"è°ƒæ•´å…°å¾·æŒ‡æ•°: {adjusted_rand_score(y, kmeans_labels):.4f}ï¼ˆè¶Šæ¥è¿‘1è¶Šå¥½ï¼‰\n")

# --------------------------
# 7. èšç±»ç»“æœå¯è§†åŒ–ä¸åˆ†æ
# --------------------------
# æ·»åŠ èšç±»æ ‡ç­¾åˆ°PCAç»“æœ
pca_df['èšç±»æ ‡ç­¾'] = kmeans_labels

# å¯è§†åŒ–ï¼šçœŸå®æ ‡ç­¾ vs èšç±»ç»“æœ
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# çœŸå®æ ‡ç­¾åˆ†å¸ƒ
axes[0].scatter(
    pca_df[pca_df['çœŸå®æ ‡ç­¾']=='ham']['PC1'],
    pca_df[pca_df['çœŸå®æ ‡ç­¾']=='ham']['PC2'],
    alpha=0.6, label='æ­£å¸¸çŸ­ä¿¡', s=30, color='#1f77b4'
)
axes[0].scatter(
    pca_df[pca_df['çœŸå®æ ‡ç­¾']=='spam']['PC1'],
    pca_df[pca_df['çœŸå®æ ‡ç­¾']=='spam']['PC2'],
    alpha=0.6, label='åƒåœ¾çŸ­ä¿¡', s=30, color='#ff7f0e'
)
axes[0].set_title('çœŸå®æ ‡ç­¾åˆ†å¸ƒ', fontsize=12)
axes[0].set_xlabel('PC1')
axes[0].set_ylabel('PC2')
axes[0].legend()
axes[0].grid(alpha=0.3)

# èšç±»ç»“æœåˆ†å¸ƒ
axes[1].scatter(
    pca_df[pca_df['èšç±»æ ‡ç­¾']==0]['PC1'],
    pca_df[pca_df['èšç±»æ ‡ç­¾']==0]['PC2'],
    alpha=0.6, label='èšç±»0', s=30, color='#1f77b4'
)
axes[1].scatter(
    pca_df[pca_df['èšç±»æ ‡ç­¾']==1]['PC1'],
    pca_df[pca_df['èšç±»æ ‡ç­¾']==1]['PC2'],
    alpha=0.6, label='èšç±»1', s=30, color='#ff7f0e'
)
axes[1].set_title('K-meansèšç±»ç»“æœ', fontsize=12)
axes[1].set_xlabel('PC1')
axes[1].set_ylabel('PC2')
axes[1].legend()
axes[1].grid(alpha=0.3)

# èšç±»ä¸­å¿ƒå¯è§†åŒ–
centers_pca = pca.transform(kmeans.cluster_centers_)
axes[2].scatter(
    pca_df['PC1'], pca_df['PC2'],
    c=pca_df['èšç±»æ ‡ç­¾'], cmap='viridis', alpha=0.3, s=30
)
axes[2].scatter(
    centers_pca[:, 0], centers_pca[:, 1],
    c='red', marker='X', s=200, label='èšç±»ä¸­å¿ƒ'
)
axes[2].set_title('èšç±»ä¸­å¿ƒå¯è§†åŒ–', fontsize=12)
axes[2].set_xlabel('PC1')
axes[2].set_ylabel('PC2')
axes[2].legend()
axes[2].grid(alpha=0.3)

plt.tight_layout()
plt.show()

# èšç±»ä¸çœŸå®æ ‡ç­¾äº¤å‰åˆ†æ
cross_tab = pd.crosstab(
    index=kmeans_labels, 
    columns=df['label'], 
    rownames=['èšç±»æ ‡ç­¾'], 
    colnames=['çœŸå®æ ‡ç­¾']
)
print("èšç±»ä¸çœŸå®æ ‡ç­¾äº¤å‰è¡¨:")
print(cross_tab)
print(f"\nèšç±»0ä¸­æ­£å¸¸çŸ­ä¿¡å æ¯”: {cross_tab.loc[0, 'ham'] / cross_tab.loc[0].sum():.2%}")
print(f"èšç±»1ä¸­åƒåœ¾çŸ­ä¿¡å æ¯”: {cross_tab.loc[1, 'spam'] / cross_tab.loc[1].sum():.2%}\n")

# æå–èšç±»å…³é”®è¯ï¼ˆæ¯ä¸ªèšç±»çš„æ ¸å¿ƒç‰¹å¾è¯ï¼‰
def get_cluster_keywords(vectorizer, kmeans_model, n_words=10):
    feature_names = vectorizer.get_feature_names_out()
    keywords = {}
    for cluster_id in range(kmeans_model.n_clusters):
        # èšç±»ä¸­å¿ƒæƒé‡æœ€é«˜çš„è¯
        top_indices = kmeans_model.cluster_centers_[cluster_id].argsort()[-n_words:][::-1]
        keywords[cluster_id] = [feature_names[idx] for idx in top_indices]
    return keywords

cluster_keywords = get_cluster_keywords(tfidf_vectorizer, kmeans)
print("å„èšç±»æ ¸å¿ƒå…³é”®è¯:")
for cluster, words in cluster_keywords.items():
    print(f"èšç±»{cluster}: {', '.join(words)}")